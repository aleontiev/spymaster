================================================================================
LeJEPA Training Summary Report
================================================================================
Generated: 2025-12-12
Checkpoint: data/checkpoints/lejepa/lejepa_best.pt (epoch 20)

================================================================================
MODEL ARCHITECTURE PARAMETERS
================================================================================

| Parameter       | Value  | Description                              |
|-----------------|--------|------------------------------------------|
| feature_dim     | 23     | Input features per timestep              |
| embedding_dim   | 256    | Latent embedding dimension               |
| num_heads       | 8      | Transformer attention heads              |
| num_layers      | 6      | Transformer encoder layers               |
| ff_dim          | 2048   | Feed-forward hidden dimension            |
| patch_length    | 30     | Timesteps per patch (30 min)             |
| context_horizon | 30     | Number of context patches                |
| target_horizon  | 30     | Number of target patches                 |

================================================================================
TRAINING HYPERPARAMETERS
================================================================================

| Parameter       | Value  | Description                              |
|-----------------|--------|------------------------------------------|
| lambda_sigreg   | 0.1    | SIGReg regularization weight             |
| ema_momentum    | 0.996  | Target encoder EMA decay rate            |
| batch_size      | 2048   | Training batch size                      |
| learning_rate   | 0.0001 | Initial learning rate (1e-4)             |
| epochs          | 20     | Total training epochs                    |
| global_step     | 3180   | Total optimization steps                 |

================================================================================
TRAINING DATA
================================================================================

| Metric              | Value                                    |
|---------------------|------------------------------------------|
| Data source         | SPY stocks + options (normalized)        |
| Date range          | 2021-12-01 to 2025-11-01                 |
| Total bars          | 382,503                                  |
| After market filter | 342,907 bars                             |
| Total days          | 984                                      |
| Training days       | 787 (80%)                                |
| Validation days     | 197 (20%)                                |

================================================================================
FINAL TRAINING METRICS (Epoch 20)
================================================================================

| Metric              | Value   | Target/Expected | Status     |
|---------------------|---------|-----------------|------------|
| Total train loss    | 6.91    | < 1.0           | HIGH       |
| Prediction loss     | 1.55    | < 0.5           | HIGH       |
| SIGReg loss         | 53.63   | ~0.0            | CRITICAL   |
| Validation loss     | 0.935   | < 0.5           | HIGH       |
| dim_variance_mean   | 0.0033  | ~1.0            | COLLAPSED  |
| dim_variance_std    | 0.0024  | < 0.5           | LOW        |

================================================================================
TRAINING PROGRESSION
================================================================================

Epoch  | Train Loss | Val Loss | Pred Loss | SIGReg Loss
-------|------------|----------|-----------|------------
1      | ~8.0       | ~1.2     | ~1.8      | ~62.0
5      | ~7.5       | ~1.0     | ~1.7      | ~58.0
10     | ~7.2       | ~0.97    | ~1.6      | ~56.0
15     | ~7.0       | ~0.94    | ~1.55     | ~54.5
20     | 6.91       | 0.935    | 1.55      | 53.63

Note: SIGReg loss remained high throughout training, indicating the
regularization weight (lambda_sigreg=0.1) was too weak to prevent collapse.

================================================================================
EMBEDDING QUALITY ANALYSIS
================================================================================

COLLAPSE DIAGNOSTICS:
- Per-dimension variance: 0.0033 (expected: ~1.0)
- Embeddings occupy <1% of available latent space
- All embeddings concentrated near origin

PREDICTIVE POWER TEST:
- Logistic regression on embeddings vs 3-class labels
- Baseline accuracy (always HOLD): 83.3%
- Embedding-based accuracy: 83.3%
- Lift over baseline: 0.0%

CONCLUSION: Embeddings have COLLAPSED and provide NO predictive value.

================================================================================
ROOT CAUSE ANALYSIS
================================================================================

1. WEAK SIGREG (Primary Issue)
   - lambda_sigreg=0.1 is too weak
   - SIGReg loss of 53.63 >> 0 shows covariance != Identity
   - Embeddings not forced to fill space

2. HIGH EMA MOMENTUM
   - ema_momentum=0.996 means target encoder updates very slowly
   - Combined with weak SIGReg, allows gradual collapse

3. OVERLAPPING HORIZONS
   - context_horizon=30, target_horizon=30 with no gap
   - May make prediction task trivially easy, reducing learning pressure

================================================================================
RECOMMENDATIONS FOR RETRAINING
================================================================================

OPTION 1: Fix SIGReg (Recommended First)
- Increase lambda_sigreg: 1.0 to 10.0
- Decrease ema_momentum: 0.98 to 0.99
- Add variance monitoring with early stopping if collapse detected

OPTION 2: Change Pretext Task
- Multi-horizon prediction (predict multiple future patches)
- Add contrastive loss component
- Reconstruction auxiliary task

OPTION 3: Change Downstream Task
- Increase lookahead beyond 30 minutes
- Target volatility prediction instead of direction
- Use regression instead of classification

OPTION 4: Skip Self-Supervised
- Train entry policy end-to-end on raw normalized features
- Simpler, avoids representation learning pitfalls
- May need larger model capacity

================================================================================
ENTRY POLICY TRAINED ON THESE EMBEDDINGS
================================================================================

The 3-class entry policy was trained on these collapsed embeddings:

| Metric           | Train  | Validation |
|------------------|--------|------------|
| Accuracy         | 83.6%  | 80.9%      |
| HOLD rate        | 83.3%  | 84.9%      |
| BUY_CALL rate    | 8.2%   | 7.6%       |
| BUY_PUT rate     | 8.5%   | 7.5%       |

BACKTEST RESULTS (using this policy):
- Total Return: -24.07%
- Total Trades: 438
- Win Rate: 28.3%
- Trade Distribution: 100% CALL, 0% PUT
- Sharpe Ratio: -4.03

The policy essentially learned to occasionally predict CALL (never PUT),
which is consistent with embeddings providing no useful signal.

================================================================================
END OF REPORT
================================================================================
